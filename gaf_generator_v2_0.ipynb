{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gaf_generator_v2.0",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%reset\n",
        "!rm -rf '/content/gaf_output'\n",
        "!rm '/content/gaf_output.zip'\n",
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvq0wLyIsMG4",
        "outputId": "bb24cec7-ab77-437e-cb0b-52dc51caea6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? n\n",
            "Nothing done.\n",
            "rm: cannot remove '/content/gaf_output.zip': No such file or directory\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "folder_name = 'gaf_output'\n",
        "training_folder_name = 'training'\n",
        "test_folder_name = 'test'\n",
        "parameter_name = 'pow'\n",
        "file_id = '1Ez3wPfDYCxVQIX_6fqt2DAf5z3YTGC-M'\n",
        "filename = 'ukdale_channel_1.dat'\n",
        "training_output_dataset_filename = 'gaf_dataset_training.csv'\n",
        "test_output_dataset_filename = 'gaf_dataset_test.csv'\n",
        "data_batch = 70000000\n",
        "GAF_RESOLUTION = 128\n",
        "IMAGE_SIZE= (9,9) # in inches\n",
        "\n",
        "NORMAL_CLASS_LABEL = 'normal'\n",
        "NORMAL_CLASS_LABEL_NO = 0\n",
        "ABNORMAL_CLASS_LABEL = 'abnormal' \n",
        "ABNORMAL_CLASS_LABEL_NO = 1\n",
        "\n",
        "eval_split = 0.1\n",
        "training_split = 0.9"
      ],
      "metadata": {
        "id": "H4mFZUV_5MIu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libaries\n",
        "!pip install -q pyts"
      ],
      "metadata": {
        "id": "1kaQlb767d8E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85d5a07-0351-47ac-f6b2-0526cfaf1bb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.5 MB 5.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from pyts.image import GramianAngularField\n",
        "from typing import *\n",
        "from multiprocessing import Pool\n",
        "import datetime as dt"
      ],
      "metadata": {
        "id": "O6N5ERKH63n8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c4gnQOLQt4B"
      },
      "source": [
        "# Import data\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "dataset_downloaded = drive.CreateFile({'id': file_id})\n",
        "dataset_downloaded.GetContentFile(filename)  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "Y-cMa-LP4XTr",
        "outputId": "4751f062-fda4-4f1b-9e85-5fc35b172d81"
      },
      "source": [
        "# Preprocessing\n",
        "col_name = ['datetime','pow']\n",
        "df = pd.read_csv(filename, names=col_name, header=None, sep=\"\\s+\")\n",
        "\n",
        "convert_dict = {'datetime': int,\n",
        "                'pow': int}\n",
        "df = df.head(data_batch)\n",
        "df = df.astype(convert_dict)\n",
        "df['datetime'] = df['datetime'].apply(lambda x: datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S'))\n",
        "print(df.size, df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4b65f1d2345a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-4b65f1d2345a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WONTs9JKo95j"
      },
      "source": [
        "# Folder setup\n",
        "print('Creating Directories...')\n",
        "\n",
        "PATH = os.path.abspath('')\n",
        "GAF_PATH = os.path.join(PATH , folder_name)\n",
        "\n",
        "TRAINING_OUTPUT_PATH = os.path.join(GAF_PATH , training_folder_name)\n",
        "TEST_OUTPUT_PATH = os.path.join(GAF_PATH , test_folder_name)\n",
        "\n",
        "TRAINING_NORMAL_PATH = os.path.join(TRAINING_OUTPUT_PATH , NORMAL_CLASS_LABEL)\n",
        "TRAINING_ABNORMAL_PATH = os.path.join(TRAINING_OUTPUT_PATH , ABNORMAL_CLASS_LABEL)\n",
        "\n",
        "os.makedirs(TRAINING_OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(TEST_OUTPUT_PATH, exist_ok=True)\n",
        "os.makedirs(TRAINING_NORMAL_PATH, exist_ok=True)\n",
        "os.makedirs(TRAINING_ABNORMAL_PATH, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxpKducNo27"
      },
      "source": [
        "# Create and save GAFs - supporting functions\n",
        "def create_gaf(ts) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    :param ts:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    data = dict()\n",
        "    gasf = GramianAngularField(method='summation', image_size=ts.shape[0])\n",
        "    data['gasf'] = gasf.fit_transform(pd.DataFrame(ts).T)[0]\n",
        "    data['gasf_mean'] = np.mean(np.mean(data['gasf'], axis=0), axis=0)\n",
        "    return data\n",
        "\n",
        "\n",
        "# Create images of the bundle that we pass\n",
        "def create_images(X_plots: Any, image_name: str, output_path: str, destination: str, image_matrix: tuple =(1, 1), mix_classes: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    :param X_plots:\n",
        "    :param image_name:\n",
        "    :param destination:\n",
        "    :param image_matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # output_dataset_image_list.append(image_name)\n",
        "    fig = plt.figure(figsize=IMAGE_SIZE)\n",
        "    grid = ImageGrid(fig,\n",
        "                     111,\n",
        "                     axes_pad=(0,0),\n",
        "                     cbar_pad=0,\n",
        "                     nrows_ncols=image_matrix,\n",
        "                     share_all=True,\n",
        "                     )\n",
        "    images = X_plots\n",
        "    for image, ax in zip(images, grid):\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.imshow(image, cmap='rainbow', origin='lower')\n",
        "\n",
        "    repo = os.path.join(output_path, destination) if mix_classes == False else output_path\n",
        "    fig.savefig(os.path.join(repo, image_name))\n",
        "    plt.close(fig)\n",
        "    return {'image_name': image_name+'.png', 'label': destination}\n",
        "\n",
        "def generate_gaf(images_data: Dict[str, pd.DataFrame]) -> None:\n",
        "    \"\"\"\n",
        "    :param images_data:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    output_dataset = pd.DataFrame(columns = ['image_name','label'])\n",
        "    main_decision = list(images_data.keys())[0]\n",
        "    for decision, data in images_data.items():\n",
        "        for i, image_data in enumerate(data):\n",
        "            # print('decision', decision)\n",
        "            # print('image_data', image_data)\n",
        "            to_plot = [create_gaf(x)['gasf'] for x in image_data[1]]\n",
        "            gaf_mean = [create_gaf(x)['gasf_mean'] for x in image_data[1]]\n",
        "            classfication = NORMAL_CLASS_LABEL if gaf_mean[0] < 0 else ABNORMAL_CLASS_LABEL\n",
        "\n",
        "            # print(i, '- gaf_mean:', gaf_mean, '-> classification:', classfication)\n",
        "            i = len(output_dataset)\n",
        "            output_path = TRAINING_OUTPUT_PATH if decision == training_folder_name else TEST_OUTPUT_PATH\n",
        "            destination_folder = classfication\n",
        "            mix_classes = False if decision == training_folder_name else True\n",
        "            output_dataset.loc[i] = create_images(X_plots=to_plot,\n",
        "                              image_name='{0}'.format(image_data[0].replace('-', '_')),\n",
        "                              output_path=output_path, destination=destination_folder, mix_classes=mix_classes)\n",
        "            \n",
        "    def label_no (row):\n",
        "      if row['label'] == NORMAL_CLASS_LABEL :\n",
        "          return NORMAL_CLASS_LABEL_NO\n",
        "      elif row['label'] == ABNORMAL_CLASS_LABEL:\n",
        "        return ABNORMAL_CLASS_LABEL_NO\n",
        "      else: \n",
        "        return ABNORMAL_CLASS_LABEL_NO\n",
        "    \n",
        "    output_dataset['label_no'] = output_dataset.apply (lambda row: label_no(row), axis=1)\n",
        "    \n",
        "    # Convert output_dataset to csv\n",
        "    dataset_filename = GAF_PATH + '/' + training_output_dataset_filename if main_decision == training_folder_name else GAF_PATH + '/' + test_output_dataset_filename\n",
        "    output_dataset.to_csv(dataset_filename, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31BK9s2HqARH"
      },
      "source": [
        "# Create and save GAFs\n",
        "def data_to_image_preprocess(data: pd.DataFrame, create_eval: bool = False, destination_folder: str = 'training') -> None:\n",
        "    \"\"\"\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    print('Processing dataset...')\n",
        "    # Drop unnecessary data_slice\n",
        "    data['DateTime'] = pd.to_datetime(data['datetime'], infer_datetime_format=True)\n",
        "    data = data.groupby(pd.Grouper(key='DateTime', freq='1h')).mean().interpolate().reset_index() \n",
        "    # print(df)\n",
        "    # Send to slicing\n",
        "    set_gaf_data(data, create_eval, destination_folder)\n",
        "\n",
        "\n",
        "def set_gaf_data(df: pd.DataFrame, create_eval: bool = False, destination_folder: str = 'training') -> None:\n",
        "    \"\"\"\n",
        "    :param df: DataFrame data_slice\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    dates = df['DateTime'].dt.date\n",
        "    dates = dates.drop_duplicates()\n",
        "    list_dates = dates.apply(str).tolist() \n",
        "    print('original list_dates:', len(list_dates))\n",
        "    eval_split_len = round(len(list_dates) * eval_split) + 1\n",
        "    training_split_len = round(len(list_dates) * training_split) + 1\n",
        "    list_dates = list_dates[:training_split_len] if create_eval == False else list_dates[:eval_split_len]\n",
        "    print('final list_dates:', len(list_dates))\n",
        "    index = 0\n",
        "    box_size = GAF_RESOLUTION\n",
        "    # Container to store data_slice for the creation of GAF\n",
        "    decision_map = {key: [] for key in [destination_folder]}\n",
        "    print('list_dates:', list_dates)\n",
        "    while True:\n",
        "        if index >= len(list_dates) - 1:\n",
        "            break\n",
        "        # Select appropriate timeframe\n",
        "        data_slice = df.loc[(df['DateTime'] < list_dates[len(list_dates) - 1]) & (df['DateTime'] > list_dates[index])]\n",
        "        # print(\"DATA SLICE==========================================\", index)\n",
        "        # print(data_slice)\n",
        "        # print(\"DATA SLICE END======================================\")\n",
        "        gafs = []\n",
        "        # Group data_slice by time frequency\n",
        "        for freq in ['1h']:\n",
        "            group_dt = data_slice.groupby(pd.Grouper(key='DateTime', freq=freq)).mean().reset_index()\n",
        "            group_dt = group_dt.dropna()\n",
        "            gafs.append(group_dt[parameter_name].head(box_size))\n",
        "            # print('group_dt', group_dt[parameter_name].head(box_size))\n",
        "        decision_map[destination_folder].append([list_dates[index], gafs])\n",
        "        index += 1\n",
        "    \n",
        "    # print('decision_map:', decision_map)\n",
        "    print('Generating GAF images...')    \n",
        "    # Generate the images from processed data_slice\n",
        "    generate_gaf(decision_map)\n",
        "    total_images = len(decision_map[destination_folder])\n",
        "    images_created = total_images\n",
        "    print(\"========GAF REPORT========:\\nTotal Images Created: {0}\".format(images_created))\n",
        "\n",
        "def main():\n",
        "    global df\n",
        "    print('CONVERTING TIME-SERIES TO GAF IMAGES...')\n",
        "    print('Training dataset:')\n",
        "    data_to_image_preprocess(data = df, create_eval = False, destination_folder = training_folder_name)\n",
        "    print(\"-----------------------------------------------Training dataset created----------------------------------\")\n",
        "    print('Test dataset:')\n",
        "    data_to_image_preprocess(data = df, create_eval = True, destination_folder = test_folder_name)\n",
        "    print(\"-----------------------------------------------Test dataset created--------------------------------------\")\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save output images as a compressed file and dataset as CSV\n",
        "!zip -r -q /content/gaf_output.zip /content/gaf_output\n",
        "files.download(\"/content/gaf_output.zip\")"
      ],
      "metadata": {
        "id": "7TlYe8SXcrCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=False)\n",
        "%cd /gdrive/My\\ Drive/Colab\\ Notebooks/data/\n",
        "%rm -rf gaf_output \n",
        "%mkdir gaf_output \n",
        "%cd gaf_output\n",
        "!unzip -q /content/gaf_output.zip"
      ],
      "metadata": {
        "id": "-n-5WKc4HLC3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}